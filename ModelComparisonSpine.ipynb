{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                    Necessary Libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jordan/miniconda2/envs/goshawk/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/Jordan/miniconda2/envs/goshawk/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#  Necessary Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cross_validation import train_test_split  \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from IPython.display import Image \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#  Initial read in\n",
    "spine = pd.read_csv('/Users/Jordan/GitHub/DataResources/Dataset_spine.csv',index_col = False ,header =None, names =['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle','sacral_slope', 'pelvic_radius',\n",
    "'degree_spondylolisthesis','pelvic_slope','direct_tilt','thoracic_slope','cervical_tilt','sacrum_angle',\n",
    "'scoliosis_slope', 'class'])\n",
    "\n",
    "#  working copy\n",
    "spine2 = spine.copy()\n",
    "spine2 = spine2.drop(0,axis =0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoded for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 310 entries, 1 to 310\n",
      "Data columns (total 13 columns):\n",
      "pelvic_incidence            310 non-null object\n",
      "pelvic_tilt                 310 non-null object\n",
      "lumbar_lordosis_angle       310 non-null object\n",
      "sacral_slope                310 non-null object\n",
      "pelvic_radius               310 non-null object\n",
      "degree_spondylolisthesis    310 non-null object\n",
      "pelvic_slope                310 non-null object\n",
      "direct_tilt                 310 non-null object\n",
      "thoracic_slope              310 non-null object\n",
      "cervical_tilt               310 non-null object\n",
      "sacrum_angle                310 non-null object\n",
      "scoliosis_slope             310 non-null object\n",
      "class                       310 non-null int64\n",
      "dtypes: int64(1), object(12)\n",
      "memory usage: 33.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pelvic_incidence</th>\n",
       "      <th>pelvic_tilt</th>\n",
       "      <th>lumbar_lordosis_angle</th>\n",
       "      <th>sacral_slope</th>\n",
       "      <th>pelvic_radius</th>\n",
       "      <th>degree_spondylolisthesis</th>\n",
       "      <th>pelvic_slope</th>\n",
       "      <th>direct_tilt</th>\n",
       "      <th>thoracic_slope</th>\n",
       "      <th>cervical_tilt</th>\n",
       "      <th>sacrum_angle</th>\n",
       "      <th>scoliosis_slope</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63.027818</td>\n",
       "      <td>22.552586</td>\n",
       "      <td>39.609117</td>\n",
       "      <td>40.475232</td>\n",
       "      <td>98.672917</td>\n",
       "      <td>-0.254400</td>\n",
       "      <td>0.744503</td>\n",
       "      <td>12.5661</td>\n",
       "      <td>14.5386</td>\n",
       "      <td>15.30468</td>\n",
       "      <td>-28.658501</td>\n",
       "      <td>43.5123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.056951</td>\n",
       "      <td>10.060991</td>\n",
       "      <td>25.015378</td>\n",
       "      <td>28.995960</td>\n",
       "      <td>114.405425</td>\n",
       "      <td>4.564259</td>\n",
       "      <td>0.415186</td>\n",
       "      <td>12.8874</td>\n",
       "      <td>17.5323</td>\n",
       "      <td>16.78486</td>\n",
       "      <td>-25.530607</td>\n",
       "      <td>16.1102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68.832021</td>\n",
       "      <td>22.218482</td>\n",
       "      <td>50.092194</td>\n",
       "      <td>46.613539</td>\n",
       "      <td>105.985135</td>\n",
       "      <td>-3.530317</td>\n",
       "      <td>0.474889</td>\n",
       "      <td>26.8343</td>\n",
       "      <td>17.4861</td>\n",
       "      <td>16.65897</td>\n",
       "      <td>-29.031888</td>\n",
       "      <td>19.2221</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.297008</td>\n",
       "      <td>24.652878</td>\n",
       "      <td>44.311238</td>\n",
       "      <td>44.644130</td>\n",
       "      <td>101.868495</td>\n",
       "      <td>11.211523</td>\n",
       "      <td>0.369345</td>\n",
       "      <td>23.5603</td>\n",
       "      <td>12.7074</td>\n",
       "      <td>11.42447</td>\n",
       "      <td>-30.470246</td>\n",
       "      <td>18.8329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49.712859</td>\n",
       "      <td>9.652075</td>\n",
       "      <td>28.317406</td>\n",
       "      <td>40.060784</td>\n",
       "      <td>108.168725</td>\n",
       "      <td>7.918501</td>\n",
       "      <td>0.543360</td>\n",
       "      <td>35.4940</td>\n",
       "      <td>15.9546</td>\n",
       "      <td>8.87237</td>\n",
       "      <td>-16.378376</td>\n",
       "      <td>24.9171</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40.250200</td>\n",
       "      <td>13.921907</td>\n",
       "      <td>25.124950</td>\n",
       "      <td>26.328293</td>\n",
       "      <td>130.327871</td>\n",
       "      <td>2.230652</td>\n",
       "      <td>0.789993</td>\n",
       "      <td>29.3230</td>\n",
       "      <td>12.0036</td>\n",
       "      <td>10.40462</td>\n",
       "      <td>-1.512209</td>\n",
       "      <td>9.6548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>53.432928</td>\n",
       "      <td>15.864336</td>\n",
       "      <td>37.165934</td>\n",
       "      <td>37.568592</td>\n",
       "      <td>120.567523</td>\n",
       "      <td>5.988551</td>\n",
       "      <td>0.198920</td>\n",
       "      <td>13.8514</td>\n",
       "      <td>10.7146</td>\n",
       "      <td>11.37832</td>\n",
       "      <td>-20.510434</td>\n",
       "      <td>25.9477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45.366754</td>\n",
       "      <td>10.755611</td>\n",
       "      <td>29.038349</td>\n",
       "      <td>34.611142</td>\n",
       "      <td>117.270068</td>\n",
       "      <td>-10.675871</td>\n",
       "      <td>0.131973</td>\n",
       "      <td>28.8165</td>\n",
       "      <td>7.7676</td>\n",
       "      <td>7.60961</td>\n",
       "      <td>-25.111459</td>\n",
       "      <td>26.3543</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>43.790190</td>\n",
       "      <td>13.533753</td>\n",
       "      <td>42.690814</td>\n",
       "      <td>30.256437</td>\n",
       "      <td>125.002893</td>\n",
       "      <td>13.289018</td>\n",
       "      <td>0.190408</td>\n",
       "      <td>22.7085</td>\n",
       "      <td>11.4234</td>\n",
       "      <td>10.59188</td>\n",
       "      <td>-20.020075</td>\n",
       "      <td>40.0276</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>36.686353</td>\n",
       "      <td>5.010884</td>\n",
       "      <td>41.948751</td>\n",
       "      <td>31.675469</td>\n",
       "      <td>84.241415</td>\n",
       "      <td>0.664437</td>\n",
       "      <td>0.367700</td>\n",
       "      <td>26.2011</td>\n",
       "      <td>8.7380</td>\n",
       "      <td>14.91416</td>\n",
       "      <td>-1.702097</td>\n",
       "      <td>21.4320</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>49.706610</td>\n",
       "      <td>13.040974</td>\n",
       "      <td>31.334500</td>\n",
       "      <td>36.665635</td>\n",
       "      <td>108.648265</td>\n",
       "      <td>-7.825986</td>\n",
       "      <td>0.688010</td>\n",
       "      <td>31.3502</td>\n",
       "      <td>16.5097</td>\n",
       "      <td>15.17645</td>\n",
       "      <td>-0.502127</td>\n",
       "      <td>18.3437</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31.232387</td>\n",
       "      <td>17.715819</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>13.516568</td>\n",
       "      <td>120.055399</td>\n",
       "      <td>0.499751</td>\n",
       "      <td>0.608343</td>\n",
       "      <td>21.4356</td>\n",
       "      <td>9.2589</td>\n",
       "      <td>14.76412</td>\n",
       "      <td>-21.724559</td>\n",
       "      <td>36.4449</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>48.915551</td>\n",
       "      <td>19.964556</td>\n",
       "      <td>40.263794</td>\n",
       "      <td>28.950995</td>\n",
       "      <td>119.321358</td>\n",
       "      <td>8.028895</td>\n",
       "      <td>0.139478</td>\n",
       "      <td>32.7916</td>\n",
       "      <td>7.2049</td>\n",
       "      <td>8.61882</td>\n",
       "      <td>-1.215542</td>\n",
       "      <td>27.3713</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>53.572170</td>\n",
       "      <td>20.460828</td>\n",
       "      <td>33.100000</td>\n",
       "      <td>33.111342</td>\n",
       "      <td>110.966698</td>\n",
       "      <td>7.044803</td>\n",
       "      <td>0.081931</td>\n",
       "      <td>15.0580</td>\n",
       "      <td>12.8127</td>\n",
       "      <td>12.00109</td>\n",
       "      <td>-1.734117</td>\n",
       "      <td>15.6205</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>57.300227</td>\n",
       "      <td>24.188885</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>33.111342</td>\n",
       "      <td>116.806587</td>\n",
       "      <td>5.766947</td>\n",
       "      <td>0.416722</td>\n",
       "      <td>16.5158</td>\n",
       "      <td>18.6222</td>\n",
       "      <td>8.51898</td>\n",
       "      <td>-33.441303</td>\n",
       "      <td>13.2498</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>44.318907</td>\n",
       "      <td>12.537992</td>\n",
       "      <td>36.098763</td>\n",
       "      <td>31.780915</td>\n",
       "      <td>124.115836</td>\n",
       "      <td>5.415825</td>\n",
       "      <td>0.664041</td>\n",
       "      <td>9.5021</td>\n",
       "      <td>19.1756</td>\n",
       "      <td>7.25707</td>\n",
       "      <td>-32.893911</td>\n",
       "      <td>19.5695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>63.834982</td>\n",
       "      <td>20.362507</td>\n",
       "      <td>54.552434</td>\n",
       "      <td>43.472475</td>\n",
       "      <td>112.309492</td>\n",
       "      <td>-0.622527</td>\n",
       "      <td>0.560675</td>\n",
       "      <td>10.7690</td>\n",
       "      <td>16.8116</td>\n",
       "      <td>11.41344</td>\n",
       "      <td>2.676002</td>\n",
       "      <td>17.3859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>31.276012</td>\n",
       "      <td>3.144669</td>\n",
       "      <td>32.562996</td>\n",
       "      <td>28.131342</td>\n",
       "      <td>129.011418</td>\n",
       "      <td>3.623020</td>\n",
       "      <td>0.534481</td>\n",
       "      <td>31.1641</td>\n",
       "      <td>18.6089</td>\n",
       "      <td>8.44020</td>\n",
       "      <td>4.482424</td>\n",
       "      <td>24.6513</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>38.697912</td>\n",
       "      <td>13.444749</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>25.253163</td>\n",
       "      <td>123.159251</td>\n",
       "      <td>1.429186</td>\n",
       "      <td>0.306581</td>\n",
       "      <td>28.3015</td>\n",
       "      <td>17.9575</td>\n",
       "      <td>14.75417</td>\n",
       "      <td>-14.252676</td>\n",
       "      <td>24.9361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>41.729963</td>\n",
       "      <td>12.254074</td>\n",
       "      <td>30.122586</td>\n",
       "      <td>29.475889</td>\n",
       "      <td>116.585706</td>\n",
       "      <td>-1.244402</td>\n",
       "      <td>0.468526</td>\n",
       "      <td>28.5598</td>\n",
       "      <td>12.4637</td>\n",
       "      <td>14.19610</td>\n",
       "      <td>-20.392538</td>\n",
       "      <td>33.0265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>43.922840</td>\n",
       "      <td>14.177959</td>\n",
       "      <td>37.832547</td>\n",
       "      <td>29.744881</td>\n",
       "      <td>134.461016</td>\n",
       "      <td>6.451648</td>\n",
       "      <td>0.280446</td>\n",
       "      <td>12.4719</td>\n",
       "      <td>16.8965</td>\n",
       "      <td>10.32658</td>\n",
       "      <td>-4.986668</td>\n",
       "      <td>22.4667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>54.919443</td>\n",
       "      <td>21.062332</td>\n",
       "      <td>42.200000</td>\n",
       "      <td>33.857110</td>\n",
       "      <td>125.212716</td>\n",
       "      <td>2.432561</td>\n",
       "      <td>0.175245</td>\n",
       "      <td>23.0791</td>\n",
       "      <td>14.2195</td>\n",
       "      <td>14.14196</td>\n",
       "      <td>3.780394</td>\n",
       "      <td>24.9278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>63.073611</td>\n",
       "      <td>24.413803</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>38.659808</td>\n",
       "      <td>106.424329</td>\n",
       "      <td>15.779697</td>\n",
       "      <td>0.666388</td>\n",
       "      <td>11.9696</td>\n",
       "      <td>17.6891</td>\n",
       "      <td>7.63771</td>\n",
       "      <td>-14.183602</td>\n",
       "      <td>44.2338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>45.540790</td>\n",
       "      <td>13.069598</td>\n",
       "      <td>30.298321</td>\n",
       "      <td>32.471192</td>\n",
       "      <td>117.980830</td>\n",
       "      <td>-4.987130</td>\n",
       "      <td>0.567450</td>\n",
       "      <td>23.8889</td>\n",
       "      <td>9.1019</td>\n",
       "      <td>7.70987</td>\n",
       "      <td>-19.379030</td>\n",
       "      <td>20.3649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>36.125683</td>\n",
       "      <td>22.758753</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>13.366931</td>\n",
       "      <td>115.577116</td>\n",
       "      <td>-3.237562</td>\n",
       "      <td>0.126474</td>\n",
       "      <td>25.6206</td>\n",
       "      <td>15.7438</td>\n",
       "      <td>11.55610</td>\n",
       "      <td>-18.108941</td>\n",
       "      <td>24.1151</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>54.124920</td>\n",
       "      <td>26.650489</td>\n",
       "      <td>35.329747</td>\n",
       "      <td>27.474432</td>\n",
       "      <td>121.447011</td>\n",
       "      <td>1.571205</td>\n",
       "      <td>0.928688</td>\n",
       "      <td>14.6686</td>\n",
       "      <td>13.5700</td>\n",
       "      <td>16.12951</td>\n",
       "      <td>-17.630363</td>\n",
       "      <td>28.1902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26.147921</td>\n",
       "      <td>10.759454</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>15.388468</td>\n",
       "      <td>125.203296</td>\n",
       "      <td>-10.093108</td>\n",
       "      <td>0.391971</td>\n",
       "      <td>9.8710</td>\n",
       "      <td>8.6406</td>\n",
       "      <td>15.78046</td>\n",
       "      <td>-19.650163</td>\n",
       "      <td>43.9550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>43.580964</td>\n",
       "      <td>16.508884</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>27.072080</td>\n",
       "      <td>109.271634</td>\n",
       "      <td>8.992816</td>\n",
       "      <td>0.594176</td>\n",
       "      <td>30.4577</td>\n",
       "      <td>17.9700</td>\n",
       "      <td>10.79356</td>\n",
       "      <td>-25.180777</td>\n",
       "      <td>18.3196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>44.551012</td>\n",
       "      <td>21.931147</td>\n",
       "      <td>26.785916</td>\n",
       "      <td>22.619865</td>\n",
       "      <td>111.072920</td>\n",
       "      <td>2.652321</td>\n",
       "      <td>0.527891</td>\n",
       "      <td>32.4275</td>\n",
       "      <td>10.2244</td>\n",
       "      <td>11.71324</td>\n",
       "      <td>-28.506125</td>\n",
       "      <td>28.0470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>66.879211</td>\n",
       "      <td>24.891999</td>\n",
       "      <td>49.278597</td>\n",
       "      <td>41.987212</td>\n",
       "      <td>113.477018</td>\n",
       "      <td>-2.005892</td>\n",
       "      <td>0.677268</td>\n",
       "      <td>12.4271</td>\n",
       "      <td>8.2495</td>\n",
       "      <td>7.58784</td>\n",
       "      <td>-3.963385</td>\n",
       "      <td>27.3587</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>50.086153</td>\n",
       "      <td>13.430044</td>\n",
       "      <td>34.457541</td>\n",
       "      <td>36.656108</td>\n",
       "      <td>119.134622</td>\n",
       "      <td>3.089484</td>\n",
       "      <td>0.744333</td>\n",
       "      <td>31.9991</td>\n",
       "      <td>15.1042</td>\n",
       "      <td>14.46625</td>\n",
       "      <td>-7.052293</td>\n",
       "      <td>15.9536</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>64.261507</td>\n",
       "      <td>14.497866</td>\n",
       "      <td>43.902504</td>\n",
       "      <td>49.763642</td>\n",
       "      <td>115.388268</td>\n",
       "      <td>5.951454</td>\n",
       "      <td>0.033703</td>\n",
       "      <td>32.2615</td>\n",
       "      <td>15.3692</td>\n",
       "      <td>16.31794</td>\n",
       "      <td>-4.179409</td>\n",
       "      <td>32.9659</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>53.683380</td>\n",
       "      <td>13.447022</td>\n",
       "      <td>41.584297</td>\n",
       "      <td>40.236358</td>\n",
       "      <td>113.913703</td>\n",
       "      <td>2.737035</td>\n",
       "      <td>0.702208</td>\n",
       "      <td>24.7795</td>\n",
       "      <td>11.4937</td>\n",
       "      <td>10.80051</td>\n",
       "      <td>-30.625194</td>\n",
       "      <td>41.9010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>48.995958</td>\n",
       "      <td>13.113820</td>\n",
       "      <td>51.873520</td>\n",
       "      <td>35.882137</td>\n",
       "      <td>126.398188</td>\n",
       "      <td>0.535472</td>\n",
       "      <td>0.732730</td>\n",
       "      <td>33.7477</td>\n",
       "      <td>7.5426</td>\n",
       "      <td>15.74090</td>\n",
       "      <td>-6.421289</td>\n",
       "      <td>11.9857</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>59.167612</td>\n",
       "      <td>14.562749</td>\n",
       "      <td>43.199158</td>\n",
       "      <td>44.604863</td>\n",
       "      <td>121.035642</td>\n",
       "      <td>2.830504</td>\n",
       "      <td>0.693906</td>\n",
       "      <td>24.8746</td>\n",
       "      <td>14.7433</td>\n",
       "      <td>8.02792</td>\n",
       "      <td>-33.650471</td>\n",
       "      <td>16.7094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>67.804694</td>\n",
       "      <td>16.550662</td>\n",
       "      <td>43.256802</td>\n",
       "      <td>51.254033</td>\n",
       "      <td>119.685645</td>\n",
       "      <td>4.867540</td>\n",
       "      <td>0.036158</td>\n",
       "      <td>18.4894</td>\n",
       "      <td>15.4016</td>\n",
       "      <td>13.86568</td>\n",
       "      <td>-17.473008</td>\n",
       "      <td>39.3526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>61.734875</td>\n",
       "      <td>17.114312</td>\n",
       "      <td>46.900000</td>\n",
       "      <td>44.620563</td>\n",
       "      <td>120.920200</td>\n",
       "      <td>3.087726</td>\n",
       "      <td>0.455056</td>\n",
       "      <td>8.8660</td>\n",
       "      <td>14.9831</td>\n",
       "      <td>8.27541</td>\n",
       "      <td>-0.488760</td>\n",
       "      <td>24.9564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>33.041688</td>\n",
       "      <td>-0.324678</td>\n",
       "      <td>19.071075</td>\n",
       "      <td>33.366366</td>\n",
       "      <td>120.388611</td>\n",
       "      <td>9.354365</td>\n",
       "      <td>0.167309</td>\n",
       "      <td>17.1960</td>\n",
       "      <td>11.2466</td>\n",
       "      <td>9.14463</td>\n",
       "      <td>-29.114560</td>\n",
       "      <td>40.9249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>74.565015</td>\n",
       "      <td>15.724320</td>\n",
       "      <td>58.618582</td>\n",
       "      <td>58.840695</td>\n",
       "      <td>105.417304</td>\n",
       "      <td>0.599247</td>\n",
       "      <td>0.117780</td>\n",
       "      <td>18.0547</td>\n",
       "      <td>15.6236</td>\n",
       "      <td>10.54562</td>\n",
       "      <td>-32.494544</td>\n",
       "      <td>16.6229</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>44.430701</td>\n",
       "      <td>14.174264</td>\n",
       "      <td>32.243495</td>\n",
       "      <td>30.256437</td>\n",
       "      <td>131.717613</td>\n",
       "      <td>-3.604255</td>\n",
       "      <td>0.126792</td>\n",
       "      <td>15.1269</td>\n",
       "      <td>7.9912</td>\n",
       "      <td>12.23055</td>\n",
       "      <td>-26.340144</td>\n",
       "      <td>32.3929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>36.422485</td>\n",
       "      <td>13.879424</td>\n",
       "      <td>20.242562</td>\n",
       "      <td>22.543061</td>\n",
       "      <td>126.076861</td>\n",
       "      <td>0.179717</td>\n",
       "      <td>0.686409</td>\n",
       "      <td>8.3909</td>\n",
       "      <td>10.8700</td>\n",
       "      <td>7.27404</td>\n",
       "      <td>-26.195988</td>\n",
       "      <td>21.8465</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>51.079833</td>\n",
       "      <td>14.209935</td>\n",
       "      <td>35.951229</td>\n",
       "      <td>36.869898</td>\n",
       "      <td>115.803711</td>\n",
       "      <td>6.905090</td>\n",
       "      <td>0.705726</td>\n",
       "      <td>24.1378</td>\n",
       "      <td>12.6652</td>\n",
       "      <td>13.92353</td>\n",
       "      <td>-3.478546</td>\n",
       "      <td>28.7425</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>34.756738</td>\n",
       "      <td>2.631740</td>\n",
       "      <td>29.504381</td>\n",
       "      <td>32.124998</td>\n",
       "      <td>127.139850</td>\n",
       "      <td>-0.460894</td>\n",
       "      <td>0.281612</td>\n",
       "      <td>24.1257</td>\n",
       "      <td>11.2762</td>\n",
       "      <td>11.54866</td>\n",
       "      <td>-12.025220</td>\n",
       "      <td>44.3412</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>48.902904</td>\n",
       "      <td>5.587589</td>\n",
       "      <td>55.500000</td>\n",
       "      <td>43.315316</td>\n",
       "      <td>137.108289</td>\n",
       "      <td>19.854759</td>\n",
       "      <td>0.215175</td>\n",
       "      <td>29.8825</td>\n",
       "      <td>9.9608</td>\n",
       "      <td>10.86798</td>\n",
       "      <td>1.956131</td>\n",
       "      <td>23.7274</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>46.236399</td>\n",
       "      <td>10.062770</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>36.173629</td>\n",
       "      <td>128.063620</td>\n",
       "      <td>-5.100053</td>\n",
       "      <td>0.860784</td>\n",
       "      <td>9.5912</td>\n",
       "      <td>15.1769</td>\n",
       "      <td>16.49989</td>\n",
       "      <td>-22.420021</td>\n",
       "      <td>40.2061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>46.426366</td>\n",
       "      <td>6.620795</td>\n",
       "      <td>48.100000</td>\n",
       "      <td>39.805571</td>\n",
       "      <td>130.350096</td>\n",
       "      <td>2.449382</td>\n",
       "      <td>0.515439</td>\n",
       "      <td>9.1955</td>\n",
       "      <td>10.6369</td>\n",
       "      <td>15.11344</td>\n",
       "      <td>2.963625</td>\n",
       "      <td>23.0719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>39.656902</td>\n",
       "      <td>16.208839</td>\n",
       "      <td>36.674857</td>\n",
       "      <td>23.448063</td>\n",
       "      <td>131.922009</td>\n",
       "      <td>-4.968980</td>\n",
       "      <td>0.794717</td>\n",
       "      <td>31.3737</td>\n",
       "      <td>18.3533</td>\n",
       "      <td>13.16102</td>\n",
       "      <td>-6.652617</td>\n",
       "      <td>26.3297</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45.575482</td>\n",
       "      <td>18.759135</td>\n",
       "      <td>33.774143</td>\n",
       "      <td>26.816347</td>\n",
       "      <td>116.797007</td>\n",
       "      <td>3.131910</td>\n",
       "      <td>0.514212</td>\n",
       "      <td>24.2526</td>\n",
       "      <td>12.9572</td>\n",
       "      <td>12.40401</td>\n",
       "      <td>-12.363109</td>\n",
       "      <td>31.9668</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>66.507179</td>\n",
       "      <td>20.897672</td>\n",
       "      <td>31.727471</td>\n",
       "      <td>45.609507</td>\n",
       "      <td>128.902905</td>\n",
       "      <td>1.517203</td>\n",
       "      <td>0.787252</td>\n",
       "      <td>12.8877</td>\n",
       "      <td>11.8978</td>\n",
       "      <td>9.23220</td>\n",
       "      <td>-14.824364</td>\n",
       "      <td>43.8409</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>82.905351</td>\n",
       "      <td>29.894119</td>\n",
       "      <td>58.250542</td>\n",
       "      <td>53.011232</td>\n",
       "      <td>110.708958</td>\n",
       "      <td>6.079338</td>\n",
       "      <td>0.827146</td>\n",
       "      <td>12.5622</td>\n",
       "      <td>12.3646</td>\n",
       "      <td>16.61754</td>\n",
       "      <td>-15.758791</td>\n",
       "      <td>35.9458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>50.676677</td>\n",
       "      <td>6.461501</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>44.215175</td>\n",
       "      <td>116.587970</td>\n",
       "      <td>-0.214711</td>\n",
       "      <td>0.021178</td>\n",
       "      <td>18.7846</td>\n",
       "      <td>8.0070</td>\n",
       "      <td>9.74352</td>\n",
       "      <td>-1.228604</td>\n",
       "      <td>14.2547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>89.014875</td>\n",
       "      <td>26.075981</td>\n",
       "      <td>69.021259</td>\n",
       "      <td>62.938894</td>\n",
       "      <td>111.481075</td>\n",
       "      <td>6.061508</td>\n",
       "      <td>0.544505</td>\n",
       "      <td>27.0219</td>\n",
       "      <td>13.3731</td>\n",
       "      <td>11.04819</td>\n",
       "      <td>-3.505300</td>\n",
       "      <td>33.4196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>54.600316</td>\n",
       "      <td>21.488974</td>\n",
       "      <td>29.360216</td>\n",
       "      <td>33.111342</td>\n",
       "      <td>118.343321</td>\n",
       "      <td>-1.471067</td>\n",
       "      <td>0.962907</td>\n",
       "      <td>30.8554</td>\n",
       "      <td>11.4198</td>\n",
       "      <td>13.82322</td>\n",
       "      <td>-5.606449</td>\n",
       "      <td>18.5514</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>34.382299</td>\n",
       "      <td>2.062683</td>\n",
       "      <td>32.390820</td>\n",
       "      <td>32.319617</td>\n",
       "      <td>128.300199</td>\n",
       "      <td>-3.365516</td>\n",
       "      <td>0.581169</td>\n",
       "      <td>12.0774</td>\n",
       "      <td>16.6255</td>\n",
       "      <td>7.20496</td>\n",
       "      <td>-31.374823</td>\n",
       "      <td>29.5748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>45.075450</td>\n",
       "      <td>12.306951</td>\n",
       "      <td>44.583177</td>\n",
       "      <td>32.768499</td>\n",
       "      <td>147.894637</td>\n",
       "      <td>-8.941709</td>\n",
       "      <td>0.932922</td>\n",
       "      <td>32.1169</td>\n",
       "      <td>14.3037</td>\n",
       "      <td>10.64326</td>\n",
       "      <td>-31.198847</td>\n",
       "      <td>11.2307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>47.903565</td>\n",
       "      <td>13.616688</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>34.286877</td>\n",
       "      <td>117.449062</td>\n",
       "      <td>-4.245395</td>\n",
       "      <td>0.129744</td>\n",
       "      <td>7.8433</td>\n",
       "      <td>14.7484</td>\n",
       "      <td>8.51707</td>\n",
       "      <td>-15.728927</td>\n",
       "      <td>11.5472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>53.936748</td>\n",
       "      <td>20.721496</td>\n",
       "      <td>29.220534</td>\n",
       "      <td>33.215251</td>\n",
       "      <td>114.365845</td>\n",
       "      <td>-0.421010</td>\n",
       "      <td>0.047913</td>\n",
       "      <td>19.1986</td>\n",
       "      <td>18.1972</td>\n",
       "      <td>7.08745</td>\n",
       "      <td>6.013843</td>\n",
       "      <td>43.8693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>61.446597</td>\n",
       "      <td>22.694968</td>\n",
       "      <td>46.170347</td>\n",
       "      <td>38.751628</td>\n",
       "      <td>125.670725</td>\n",
       "      <td>-2.707880</td>\n",
       "      <td>0.081070</td>\n",
       "      <td>16.2059</td>\n",
       "      <td>13.5565</td>\n",
       "      <td>8.89572</td>\n",
       "      <td>3.564463</td>\n",
       "      <td>18.4151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>45.252792</td>\n",
       "      <td>8.693157</td>\n",
       "      <td>41.583126</td>\n",
       "      <td>36.559635</td>\n",
       "      <td>118.545842</td>\n",
       "      <td>0.214750</td>\n",
       "      <td>0.159251</td>\n",
       "      <td>14.7334</td>\n",
       "      <td>16.0928</td>\n",
       "      <td>9.75922</td>\n",
       "      <td>5.767308</td>\n",
       "      <td>33.7192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>33.841641</td>\n",
       "      <td>5.073991</td>\n",
       "      <td>36.641233</td>\n",
       "      <td>28.767649</td>\n",
       "      <td>123.945244</td>\n",
       "      <td>-0.199249</td>\n",
       "      <td>0.674504</td>\n",
       "      <td>19.3825</td>\n",
       "      <td>17.6963</td>\n",
       "      <td>13.72929</td>\n",
       "      <td>1.783007</td>\n",
       "      <td>40.6049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pelvic_incidence  pelvic_tilt  lumbar_lordosis_angle  sacral_slope  \\\n",
       "1           63.027818    22.552586              39.609117     40.475232   \n",
       "2           39.056951    10.060991              25.015378     28.995960   \n",
       "3           68.832021    22.218482              50.092194     46.613539   \n",
       "4           69.297008    24.652878              44.311238     44.644130   \n",
       "5           49.712859     9.652075              28.317406     40.060784   \n",
       "6           40.250200    13.921907              25.124950     26.328293   \n",
       "7           53.432928    15.864336              37.165934     37.568592   \n",
       "8           45.366754    10.755611              29.038349     34.611142   \n",
       "9           43.790190    13.533753              42.690814     30.256437   \n",
       "10          36.686353     5.010884              41.948751     31.675469   \n",
       "11          49.706610    13.040974              31.334500     36.665635   \n",
       "12          31.232387    17.715819              15.500000     13.516568   \n",
       "13          48.915551    19.964556              40.263794     28.950995   \n",
       "14          53.572170    20.460828              33.100000     33.111342   \n",
       "15          57.300227    24.188885              47.000000     33.111342   \n",
       "16          44.318907    12.537992              36.098763     31.780915   \n",
       "17          63.834982    20.362507              54.552434     43.472475   \n",
       "18          31.276012     3.144669              32.562996     28.131342   \n",
       "19          38.697912    13.444749              31.000000     25.253163   \n",
       "20          41.729963    12.254074              30.122586     29.475889   \n",
       "21          43.922840    14.177959              37.832547     29.744881   \n",
       "22          54.919443    21.062332              42.200000     33.857110   \n",
       "23          63.073611    24.413803              54.000000     38.659808   \n",
       "24          45.540790    13.069598              30.298321     32.471192   \n",
       "25          36.125683    22.758753              29.000000     13.366931   \n",
       "26          54.124920    26.650489              35.329747     27.474432   \n",
       "27          26.147921    10.759454              14.000000     15.388468   \n",
       "28          43.580964    16.508884              47.000000     27.072080   \n",
       "29          44.551012    21.931147              26.785916     22.619865   \n",
       "30          66.879211    24.891999              49.278597     41.987212   \n",
       "..                ...          ...                    ...           ...   \n",
       "281         50.086153    13.430044              34.457541     36.656108   \n",
       "282         64.261507    14.497866              43.902504     49.763642   \n",
       "283         53.683380    13.447022              41.584297     40.236358   \n",
       "284         48.995958    13.113820              51.873520     35.882137   \n",
       "285         59.167612    14.562749              43.199158     44.604863   \n",
       "286         67.804694    16.550662              43.256802     51.254033   \n",
       "287         61.734875    17.114312              46.900000     44.620563   \n",
       "288         33.041688    -0.324678              19.071075     33.366366   \n",
       "289         74.565015    15.724320              58.618582     58.840695   \n",
       "290         44.430701    14.174264              32.243495     30.256437   \n",
       "291         36.422485    13.879424              20.242562     22.543061   \n",
       "292         51.079833    14.209935              35.951229     36.869898   \n",
       "293         34.756738     2.631740              29.504381     32.124998   \n",
       "294         48.902904     5.587589              55.500000     43.315316   \n",
       "295         46.236399    10.062770              37.000000     36.173629   \n",
       "296         46.426366     6.620795              48.100000     39.805571   \n",
       "297         39.656902    16.208839              36.674857     23.448063   \n",
       "298         45.575482    18.759135              33.774143     26.816347   \n",
       "299         66.507179    20.897672              31.727471     45.609507   \n",
       "300         82.905351    29.894119              58.250542     53.011232   \n",
       "301         50.676677     6.461501              35.000000     44.215175   \n",
       "302         89.014875    26.075981              69.021259     62.938894   \n",
       "303         54.600316    21.488974              29.360216     33.111342   \n",
       "304         34.382299     2.062683              32.390820     32.319617   \n",
       "305         45.075450    12.306951              44.583177     32.768499   \n",
       "306         47.903565    13.616688              36.000000     34.286877   \n",
       "307         53.936748    20.721496              29.220534     33.215251   \n",
       "308         61.446597    22.694968              46.170347     38.751628   \n",
       "309         45.252792     8.693157              41.583126     36.559635   \n",
       "310         33.841641     5.073991              36.641233     28.767649   \n",
       "\n",
       "     pelvic_radius  degree_spondylolisthesis  pelvic_slope  direct_tilt  \\\n",
       "1        98.672917                 -0.254400      0.744503      12.5661   \n",
       "2       114.405425                  4.564259      0.415186      12.8874   \n",
       "3       105.985135                 -3.530317      0.474889      26.8343   \n",
       "4       101.868495                 11.211523      0.369345      23.5603   \n",
       "5       108.168725                  7.918501      0.543360      35.4940   \n",
       "6       130.327871                  2.230652      0.789993      29.3230   \n",
       "7       120.567523                  5.988551      0.198920      13.8514   \n",
       "8       117.270068                -10.675871      0.131973      28.8165   \n",
       "9       125.002893                 13.289018      0.190408      22.7085   \n",
       "10       84.241415                  0.664437      0.367700      26.2011   \n",
       "11      108.648265                 -7.825986      0.688010      31.3502   \n",
       "12      120.055399                  0.499751      0.608343      21.4356   \n",
       "13      119.321358                  8.028895      0.139478      32.7916   \n",
       "14      110.966698                  7.044803      0.081931      15.0580   \n",
       "15      116.806587                  5.766947      0.416722      16.5158   \n",
       "16      124.115836                  5.415825      0.664041       9.5021   \n",
       "17      112.309492                 -0.622527      0.560675      10.7690   \n",
       "18      129.011418                  3.623020      0.534481      31.1641   \n",
       "19      123.159251                  1.429186      0.306581      28.3015   \n",
       "20      116.585706                 -1.244402      0.468526      28.5598   \n",
       "21      134.461016                  6.451648      0.280446      12.4719   \n",
       "22      125.212716                  2.432561      0.175245      23.0791   \n",
       "23      106.424329                 15.779697      0.666388      11.9696   \n",
       "24      117.980830                 -4.987130      0.567450      23.8889   \n",
       "25      115.577116                 -3.237562      0.126474      25.6206   \n",
       "26      121.447011                  1.571205      0.928688      14.6686   \n",
       "27      125.203296                -10.093108      0.391971       9.8710   \n",
       "28      109.271634                  8.992816      0.594176      30.4577   \n",
       "29      111.072920                  2.652321      0.527891      32.4275   \n",
       "30      113.477018                 -2.005892      0.677268      12.4271   \n",
       "..             ...                       ...           ...          ...   \n",
       "281     119.134622                  3.089484      0.744333      31.9991   \n",
       "282     115.388268                  5.951454      0.033703      32.2615   \n",
       "283     113.913703                  2.737035      0.702208      24.7795   \n",
       "284     126.398188                  0.535472      0.732730      33.7477   \n",
       "285     121.035642                  2.830504      0.693906      24.8746   \n",
       "286     119.685645                  4.867540      0.036158      18.4894   \n",
       "287     120.920200                  3.087726      0.455056       8.8660   \n",
       "288     120.388611                  9.354365      0.167309      17.1960   \n",
       "289     105.417304                  0.599247      0.117780      18.0547   \n",
       "290     131.717613                 -3.604255      0.126792      15.1269   \n",
       "291     126.076861                  0.179717      0.686409       8.3909   \n",
       "292     115.803711                  6.905090      0.705726      24.1378   \n",
       "293     127.139850                 -0.460894      0.281612      24.1257   \n",
       "294     137.108289                 19.854759      0.215175      29.8825   \n",
       "295     128.063620                 -5.100053      0.860784       9.5912   \n",
       "296     130.350096                  2.449382      0.515439       9.1955   \n",
       "297     131.922009                 -4.968980      0.794717      31.3737   \n",
       "298     116.797007                  3.131910      0.514212      24.2526   \n",
       "299     128.902905                  1.517203      0.787252      12.8877   \n",
       "300     110.708958                  6.079338      0.827146      12.5622   \n",
       "301     116.587970                 -0.214711      0.021178      18.7846   \n",
       "302     111.481075                  6.061508      0.544505      27.0219   \n",
       "303     118.343321                 -1.471067      0.962907      30.8554   \n",
       "304     128.300199                 -3.365516      0.581169      12.0774   \n",
       "305     147.894637                 -8.941709      0.932922      32.1169   \n",
       "306     117.449062                 -4.245395      0.129744       7.8433   \n",
       "307     114.365845                 -0.421010      0.047913      19.1986   \n",
       "308     125.670725                 -2.707880      0.081070      16.2059   \n",
       "309     118.545842                  0.214750      0.159251      14.7334   \n",
       "310     123.945244                 -0.199249      0.674504      19.3825   \n",
       "\n",
       "     thoracic_slope  cervical_tilt  sacrum_angle  scoliosis_slope  class  \n",
       "1           14.5386       15.30468    -28.658501          43.5123      1  \n",
       "2           17.5323       16.78486    -25.530607          16.1102      1  \n",
       "3           17.4861       16.65897    -29.031888          19.2221      1  \n",
       "4           12.7074       11.42447    -30.470246          18.8329      1  \n",
       "5           15.9546        8.87237    -16.378376          24.9171      1  \n",
       "6           12.0036       10.40462     -1.512209           9.6548      1  \n",
       "7           10.7146       11.37832    -20.510434          25.9477      1  \n",
       "8            7.7676        7.60961    -25.111459          26.3543      1  \n",
       "9           11.4234       10.59188    -20.020075          40.0276      1  \n",
       "10           8.7380       14.91416     -1.702097          21.4320      1  \n",
       "11          16.5097       15.17645     -0.502127          18.3437      1  \n",
       "12           9.2589       14.76412    -21.724559          36.4449      1  \n",
       "13           7.2049        8.61882     -1.215542          27.3713      1  \n",
       "14          12.8127       12.00109     -1.734117          15.6205      1  \n",
       "15          18.6222        8.51898    -33.441303          13.2498      1  \n",
       "16          19.1756        7.25707    -32.893911          19.5695      1  \n",
       "17          16.8116       11.41344      2.676002          17.3859      1  \n",
       "18          18.6089        8.44020      4.482424          24.6513      1  \n",
       "19          17.9575       14.75417    -14.252676          24.9361      1  \n",
       "20          12.4637       14.19610    -20.392538          33.0265      1  \n",
       "21          16.8965       10.32658     -4.986668          22.4667      1  \n",
       "22          14.2195       14.14196      3.780394          24.9278      1  \n",
       "23          17.6891        7.63771    -14.183602          44.2338      1  \n",
       "24           9.1019        7.70987    -19.379030          20.3649      1  \n",
       "25          15.7438       11.55610    -18.108941          24.1151      1  \n",
       "26          13.5700       16.12951    -17.630363          28.1902      1  \n",
       "27           8.6406       15.78046    -19.650163          43.9550      1  \n",
       "28          17.9700       10.79356    -25.180777          18.3196      1  \n",
       "29          10.2244       11.71324    -28.506125          28.0470      1  \n",
       "30           8.2495        7.58784     -3.963385          27.3587      1  \n",
       "..              ...            ...           ...              ...    ...  \n",
       "281         15.1042       14.46625     -7.052293          15.9536      0  \n",
       "282         15.3692       16.31794     -4.179409          32.9659      0  \n",
       "283         11.4937       10.80051    -30.625194          41.9010      0  \n",
       "284          7.5426       15.74090     -6.421289          11.9857      0  \n",
       "285         14.7433        8.02792    -33.650471          16.7094      0  \n",
       "286         15.4016       13.86568    -17.473008          39.3526      0  \n",
       "287         14.9831        8.27541     -0.488760          24.9564      0  \n",
       "288         11.2466        9.14463    -29.114560          40.9249      0  \n",
       "289         15.6236       10.54562    -32.494544          16.6229      0  \n",
       "290          7.9912       12.23055    -26.340144          32.3929      0  \n",
       "291         10.8700        7.27404    -26.195988          21.8465      0  \n",
       "292         12.6652       13.92353     -3.478546          28.7425      0  \n",
       "293         11.2762       11.54866    -12.025220          44.3412      0  \n",
       "294          9.9608       10.86798      1.956131          23.7274      0  \n",
       "295         15.1769       16.49989    -22.420021          40.2061      0  \n",
       "296         10.6369       15.11344      2.963625          23.0719      0  \n",
       "297         18.3533       13.16102     -6.652617          26.3297      0  \n",
       "298         12.9572       12.40401    -12.363109          31.9668      0  \n",
       "299         11.8978        9.23220    -14.824364          43.8409      0  \n",
       "300         12.3646       16.61754    -15.758791          35.9458      0  \n",
       "301          8.0070        9.74352     -1.228604          14.2547      0  \n",
       "302         13.3731       11.04819     -3.505300          33.4196      0  \n",
       "303         11.4198       13.82322     -5.606449          18.5514      0  \n",
       "304         16.6255        7.20496    -31.374823          29.5748      0  \n",
       "305         14.3037       10.64326    -31.198847          11.2307      0  \n",
       "306         14.7484        8.51707    -15.728927          11.5472      0  \n",
       "307         18.1972        7.08745      6.013843          43.8693      0  \n",
       "308         13.5565        8.89572      3.564463          18.4151      0  \n",
       "309         16.0928        9.75922      5.767308          33.7192      0  \n",
       "310         17.6963       13.72929      1.783007          40.6049      0  \n",
       "\n",
       "[310 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "le = {'Abnormal': 1, 'Normal': 0}\n",
    "spine2['class'] = spine2['class'].map(le)\n",
    "features = [c for c in spine2.columns if c != 'class']\n",
    "spine2.info()\n",
    "spine2.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 310 entries, 1 to 310\n",
      "Data columns (total 13 columns):\n",
      "pelvic_incidence            310 non-null object\n",
      "pelvic_tilt                 310 non-null object\n",
      "lumbar_lordosis_angle       310 non-null object\n",
      "sacral_slope                310 non-null object\n",
      "pelvic_radius               310 non-null object\n",
      "degree_spondylolisthesis    310 non-null object\n",
      "pelvic_slope                310 non-null object\n",
      "direct_tilt                 310 non-null object\n",
      "thoracic_slope              310 non-null object\n",
      "cervical_tilt               310 non-null object\n",
      "sacrum_angle                310 non-null object\n",
      "scoliosis_slope             310 non-null object\n",
      "class                       310 non-null int64\n",
      "dtypes: int64(1), object(12)\n",
      "memory usage: 33.9+ KB\n"
     ]
    }
   ],
   "source": [
    "spine2.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spine data information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Grouped Variables \n",
    "\n",
    "y = spine2['class']\n",
    "X = spine2[features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "\n",
    "# Scaled\n",
    "\n",
    "Xs = preprocessing.scale(X)\n",
    "ys =y\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, ys, test_size=0.3, random_state = 42)\n",
    "                                                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f(x) to evaluate each mode\n",
    "\n",
    "\n",
    "def evaluate_model(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    a = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cr = classification_report(y_test, y_pred)\n",
    "\n",
    "    print model\n",
    "    print cm\n",
    "    print cr\n",
    "    \n",
    "    return a\n",
    "\n",
    "\n",
    "# scaled\n",
    "def evaluate_model_s(model):\n",
    "    model.fit(Xs_train, ys_train)\n",
    "    ys_pred = model.predict(Xs_test)\n",
    "    \n",
    "    a = accuracy_score(ys_test, ys_pred)\n",
    "    cm = confusion_matrix(ys_test, ys_pred)\n",
    "    cr = classification_report(ys_test, ys_pred)\n",
    "    print model\n",
    "    print cm\n",
    "    print cr\n",
    "    return a \n",
    "\n",
    "\n",
    "all_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "[[17  7]\n",
      " [ 9 60]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.71      0.68        24\n",
      "          1       0.90      0.87      0.88        69\n",
      "\n",
      "avg / total       0.83      0.83      0.83        93\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "[[18  6]\n",
      " [13 56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.75      0.65        24\n",
      "          1       0.90      0.81      0.85        69\n",
      "\n",
      "avg / total       0.82      0.80      0.80        93\n",
      "\n",
      "0.827956989247\n",
      "0.795698924731\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  KNN Classifer\n",
    "knn_mod = evaluate_model(KNeighborsClassifier())\n",
    "knn_mod_s= evaluate_model_s(KNeighborsClassifier())\n",
    "\n",
    "print knn_mod\n",
    "print knn_mod_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=22, p=2,\n",
      "           weights='uniform')\n",
      "[[21  3]\n",
      " [13 56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.88      0.72        24\n",
      "          1       0.95      0.81      0.87        69\n",
      "\n",
      "avg / total       0.86      0.83      0.84        93\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=18, p=2,\n",
      "           weights='uniform')\n",
      "[[20  4]\n",
      " [14 55]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.83      0.69        24\n",
      "          1       0.93      0.80      0.86        69\n",
      "\n",
      "avg / total       0.84      0.81      0.82        93\n",
      "\n",
      "scaled 0.806451612903\n",
      "not scaled 0.827956989247\n"
     ]
    }
   ],
   "source": [
    "#  Model Evaulation with KNN, best parameters found with grid search\n",
    "params = {'n_neighbors': range(2,60)}\n",
    "\n",
    "gsknn = GridSearchCV(KNeighborsClassifier(),\n",
    "                     params, n_jobs=-1,\n",
    "                     cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "# normal \n",
    "gsknn.fit(X, y)\n",
    "a1 = gsknn.best_params_\n",
    "gsknn_mod = gsknn.best_score_\n",
    "a =evaluate_model(gsknn.best_estimator_)\n",
    "\n",
    "\n",
    "\n",
    "# scaled\n",
    "gsknn.fit(Xs,ys)\n",
    "a2= gsknn.best_params_\n",
    "gsknn_s_mod = gsknn.best_score_\n",
    "b1 = evaluate_model_s(gsknn.best_estimator_)\n",
    "print \"scaled\" ,b1\n",
    "print \"not scaled\" ,a\n",
    "# GridSearch \n",
    "all_models['gsknn'] = {'model': gsknn.best_estimator_,\n",
    "                     'score': a}\n",
    "# GridSearch Scaled\n",
    "all_models['gsknn_s'] = {'model': gsknn.best_estimator_,\n",
    "                     'score': b1}\n",
    "\n",
    "# KNN\n",
    "all_models['knn'] = {'model': KNeighborsClassifier(),\n",
    "                     'score': knn_mod}\n",
    "# KNN scaled\n",
    "all_models['knn_s'] = {'model': KNeighborsClassifier(),\n",
    "                     'score': knn_mod_s}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging knn BaggingClassifier(base_estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=1.0, n_estimators=10, n_jobs=1, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)\n",
      "[[19  5]\n",
      " [ 9 60]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.79      0.73        24\n",
      "          1       0.92      0.87      0.90        69\n",
      "\n",
      "avg / total       0.86      0.85      0.85        93\n",
      "\n",
      "0.849462365591\n"
     ]
    }
   ],
   "source": [
    "# Bagging KNN \n",
    "\n",
    "baggingknn = BaggingClassifier(KNeighborsClassifier())\n",
    "\n",
    "print 'bagging knn' ,evaluate_model(baggingknn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grid Searched bagged knn for best parameters\n",
    "\n",
    "bagging_params = {'n_estimators': [10, 20],\n",
    "                  'max_samples': [0.7, 1.0],\n",
    "                  'max_features': [0.7, 1.0],\n",
    "                  'bootstrap_features': [True, False]}\n",
    "\n",
    "\n",
    "gsbaggingknn = GridSearchCV(baggingknn,\n",
    "                            bagging_params, n_jobs=-1,\n",
    "                            cv=KFold(len(y), n_folds=3, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=1.0, n_estimators=20, n_jobs=1, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)\n",
      "[[17  7]\n",
      " [10 59]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.71      0.67        24\n",
      "          1       0.89      0.86      0.87        69\n",
      "\n",
      "avg / total       0.83      0.82      0.82        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsbaggingknn.fit(X,y)\n",
    "gsbaggingknn.best_params_\n",
    "\n",
    "all_models['gsbaggingknn'] = {'model': gsbaggingknn.best_estimator_,\n",
    "                              'score': evaluate_model(gsbaggingknn.best_estimator_)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "[[19  5]\n",
      " [ 8 61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.79      0.75        24\n",
      "          1       0.92      0.88      0.90        69\n",
      "\n",
      "avg / total       0.87      0.86      0.86        93\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "[[20  4]\n",
      " [ 5 64]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.83      0.82        24\n",
      "          1       0.94      0.93      0.93        69\n",
      "\n",
      "avg / total       0.90      0.90      0.90        93\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "[[19  5]\n",
      " [ 8 61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.79      0.75        24\n",
      "          1       0.92      0.88      0.90        69\n",
      "\n",
      "avg / total       0.87      0.86      0.86        93\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "[[20  4]\n",
      " [ 5 64]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.83      0.82        24\n",
      "          1       0.94      0.93      0.93        69\n",
      "\n",
      "avg / total       0.90      0.90      0.90        93\n",
      "\n",
      "not scaled 0.860215053763\n",
      "scaled 0.903225806452\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "a= evaluate_model(lr)\n",
    "b = evaluate_model_s(lr)\n",
    "all_models['lr'] = {'model': lr,\n",
    "                    'score': evaluate_model(lr)}\n",
    "all_models['lr_s'] = {'model': lr,\n",
    "                    'score': evaluate_model_s(lr)}\n",
    "print \"not scaled\",a\n",
    "print \"scaled\" ,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'C': 10.0}\n",
      "0.848387096774\n",
      "LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "[[18  6]\n",
      " [ 6 63]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.75      0.75        24\n",
      "          1       0.91      0.91      0.91        69\n",
      "\n",
      "avg / total       0.87      0.87      0.87        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "          'penalty': ['l1', 'l2']}\n",
    "\n",
    "gslr = GridSearchCV(lr,\n",
    "                    params, n_jobs=-1,\n",
    "                    cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "# not scaled \n",
    "gslr.fit(X, y)\n",
    "print gslr.best_params_\n",
    "print gslr.best_score_\n",
    "\n",
    "all_models['gslr'] = {'model': gslr.best_estimator_,\n",
    "                             'score': evaluate_model(gslr.best_estimator_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l2', 'C': 10.0}\n",
      "0.854838709677\n",
      "LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "[[18  6]\n",
      " [ 4 65]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.75      0.78        24\n",
      "          1       0.92      0.94      0.93        69\n",
      "\n",
      "avg / total       0.89      0.89      0.89        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scaled\n",
    "gslr.fit(Xs, y)\n",
    "print gslr.best_params_\n",
    "print gslr.best_score_\n",
    "\n",
    "\n",
    "all_models['gslr_s'] = {'model': lr,\n",
    "                    'score': evaluate_model_s(gslr.best_estimator_)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 1.0, 'max_samples': 0.7, 'n_estimators': 20, 'bootstrap_features': False}\n",
      "0.845161290323\n",
      "BaggingClassifier(base_estimator=LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=0.7, n_estimators=20, n_jobs=1, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)\n",
      "[[19  5]\n",
      " [ 8 61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.79      0.75        24\n",
      "          1       0.92      0.88      0.90        69\n",
      "\n",
      "avg / total       0.87      0.86      0.86        93\n",
      "\n",
      "BaggingClassifier(base_estimator=LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=0.7, n_estimators=20, n_jobs=1, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)\n",
      "[[18  6]\n",
      " [ 6 63]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.75      0.75        24\n",
      "          1       0.91      0.91      0.91        69\n",
      "\n",
      "avg / total       0.87      0.87      0.87        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gsbagginglr = GridSearchCV(BaggingClassifier(gslr.best_estimator_),\n",
    "                           bagging_params, n_jobs=-1,\n",
    "                           cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gsbagginglr.fit(X, y)\n",
    "\n",
    "print gsbagginglr.best_params_\n",
    "print gsbagginglr.best_score_\n",
    "\n",
    "all_models['gsbagginglr'] = {'model': gsbagginglr.best_estimator_,\n",
    "                             'score': evaluate_model(gsbagginglr.best_estimator_)}\n",
    "\n",
    "all_models['gsbagginglr'] = {'model': gsbagginglr.best_estimator_,\n",
    "                             'score': evaluate_model_s(gsbagginglr.best_estimator_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "[[13 11]\n",
      " [10 59]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.54      0.55        24\n",
      "          1       0.84      0.86      0.85        69\n",
      "\n",
      "avg / total       0.77      0.77      0.77        93\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "[[13 11]\n",
      " [10 59]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.54      0.55        24\n",
      "          1       0.84      0.86      0.85        69\n",
      "\n",
      "avg / total       0.77      0.77      0.77        93\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "[[12 12]\n",
      " [10 59]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.50      0.52        24\n",
      "          1       0.83      0.86      0.84        69\n",
      "\n",
      "avg / total       0.76      0.76      0.76        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "evaluate_model(dt)\n",
    "\n",
    "all_models['dt'] = {'model': dt,\n",
    "                    'score': evaluate_model(dt)}\n",
    "all_models['dt_s'] = {'model': dt,\n",
    "                             'score': evaluate_model_s(dt)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 2, 'splitter': 'best', 'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 3}\n",
      "0.825806451613\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=3,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "[[12 12]\n",
      " [ 8 61]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.50      0.55        24\n",
      "          1       0.84      0.88      0.86        69\n",
      "\n",
      "avg / total       0.77      0.78      0.78        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'splitter': ['best', 'random'],\n",
    "          'max_depth': [None, 5, 10],\n",
    "          'min_samples_split': [2, 5],\n",
    "          'min_samples_leaf': [1, 2, 3]}\n",
    "\n",
    "gsdt = GridSearchCV(dt,\n",
    "                    params, n_jobs=-1,\n",
    "                    cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gsdt.fit(X, y)\n",
    "print gsdt.best_params_\n",
    "print gsdt.best_score_\n",
    "\n",
    "all_models['gsdt'] = {'model': gsdt.best_estimator_,\n",
    "                      'score': evaluate_model(gsdt.best_estimator_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 5, 'splitter': 'best', 'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 3}\n",
      "0.832258064516\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=3,\n",
      "            min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "[[13 11]\n",
      " [12 57]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.54      0.53        24\n",
      "          1       0.84      0.83      0.83        69\n",
      "\n",
      "avg / total       0.76      0.75      0.75        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gsdt.fit(Xs, y)\n",
    "print gsdt.best_params_\n",
    "print gsdt.best_score_\n",
    "\n",
    "all_models['gsdt_s'] = {'model': gsdt.best_estimator_,\n",
    "                      'score': evaluate_model_s(gsdt.best_estimator_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr_s</th>\n",
       "      <td>0.903226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gslr_s</th>\n",
       "      <td>0.892473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gslr</th>\n",
       "      <td>0.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsbagginglr</th>\n",
       "      <td>0.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>0.860215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.827957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsknn</th>\n",
       "      <td>0.827957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsbaggingknn</th>\n",
       "      <td>0.817204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsknn_s</th>\n",
       "      <td>0.806452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn_s</th>\n",
       "      <td>0.795699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsdt</th>\n",
       "      <td>0.784946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt_s</th>\n",
       "      <td>0.763441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsdt_s</th>\n",
       "      <td>0.752688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 score\n",
       "model                 \n",
       "lr_s          0.903226\n",
       "gslr_s        0.892473\n",
       "gslr          0.870968\n",
       "gsbagginglr   0.870968\n",
       "lr            0.860215\n",
       "knn           0.827957\n",
       "gsknn         0.827957\n",
       "gsbaggingknn  0.817204\n",
       "gsknn_s       0.806452\n",
       "knn_s         0.795699\n",
       "gsdt          0.784946\n",
       "dt            0.774194\n",
       "dt_s          0.763441\n",
       "gsdt_s        0.752688"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAFTCAYAAADC/UzeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmSXo/wNr0y7qaZ9vYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1140ea910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "scores = pd.DataFrame([(k, v['score']) for k, v in all_models.iteritems()],\n",
    "             columns=['model', 'score']).set_index('model').sort_values('score', ascending=False)\n",
    "\n",
    "\n",
    "scores.plot(kind='bar')\n",
    "plt.title(\"Test Train Split\")\n",
    "plt.ylim(0.6, 1.1)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsknn_s knn gsdt_s gsknn gslr_s gslr gsdt knn_s dt_s lr gsbaggingknn gsbagginglr lr_s dt"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gsbagginglr</th>\n",
       "      <td>0.858041</td>\n",
       "      <td>0.005193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn_s</th>\n",
       "      <td>0.845283</td>\n",
       "      <td>0.030950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gslr</th>\n",
       "      <td>0.845158</td>\n",
       "      <td>0.031716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gslr_s</th>\n",
       "      <td>0.832151</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsbaggingknn</th>\n",
       "      <td>0.829039</td>\n",
       "      <td>0.019874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>0.822598</td>\n",
       "      <td>0.016290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsknn_s</th>\n",
       "      <td>0.819548</td>\n",
       "      <td>0.042564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr_s</th>\n",
       "      <td>0.816094</td>\n",
       "      <td>0.008622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.809497</td>\n",
       "      <td>0.044209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsknn</th>\n",
       "      <td>0.803180</td>\n",
       "      <td>0.012753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>0.790329</td>\n",
       "      <td>0.004178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt_s</th>\n",
       "      <td>0.787123</td>\n",
       "      <td>0.013253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsdt</th>\n",
       "      <td>0.774241</td>\n",
       "      <td>0.037141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gsdt_s</th>\n",
       "      <td>0.774210</td>\n",
       "      <td>0.039792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 score     error\n",
       "model                           \n",
       "gsbagginglr   0.858041  0.005193\n",
       "knn_s         0.845283  0.030950\n",
       "gslr          0.845158  0.031716\n",
       "gslr_s        0.832151  0.023500\n",
       "gsbaggingknn  0.829039  0.019874\n",
       "lr            0.822598  0.016290\n",
       "gsknn_s       0.819548  0.042564\n",
       "lr_s          0.816094  0.008622\n",
       "knn           0.809497  0.044209\n",
       "gsknn         0.803180  0.012753\n",
       "dt            0.790329  0.004178\n",
       "dt_s          0.787123  0.013253\n",
       "gsdt          0.774241  0.037141\n",
       "gsdt_s        0.774210  0.039792"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAFECAYAAADIlyJZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11005fb90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def retest(model):\n",
    "    scores = cross_val_score(model, X, y,\n",
    "                             cv=StratifiedKFold(y, shuffle=True),\n",
    "                             n_jobs=-1)\n",
    "    m = scores.mean()\n",
    "    s = scores.std()\n",
    "    \n",
    "    return m, s\n",
    "\n",
    "for k, v in all_models.iteritems():\n",
    "    cvres = retest(v['model'])\n",
    "    print k, \n",
    "    all_models[k]['cvres'] = cvres\n",
    "    \n",
    "cvscores = pd.DataFrame([(k, v['cvres'][0], v['cvres'][1] ) for k, v in all_models.iteritems()],\n",
    "                        columns=['model', 'score', 'error']).set_index('model').sort_values('score', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(range(len(cvscores)), cvscores.score,\n",
    "                yerr=cvscores.error,\n",
    "                tick_label=cvscores.index)\n",
    "plt.title(\"Stratifed KFold CV\")\n",
    "ax.set_ylabel('Scores')\n",
    "plt.xticks(rotation=70)\n",
    "plt.ylim(0.6, 1.1)\n",
    "\n",
    "cvscores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:goshawk]",
   "language": "python",
   "name": "conda-env-goshawk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
